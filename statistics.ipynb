{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill # pickle but for more complex objects (for saving the stats_dict which contains the custom OLS_Stats namedtuple)\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import norm, kendalltau, theilslopes, zscore, f_oneway, levene\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = namedtuple(\"coordinates\", [\"lat\", \"lon\"])\n",
    "\n",
    "@dataclass\n",
    "class Site_Data:\n",
    "    elevation: int\n",
    "    coords: coordinates = None\n",
    "    sat_day: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    sat_month: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    sat_season: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    sat_annual: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    cli_day: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    cli_month: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    cli_season: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    cli_annual: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "\n",
    "@dataclass\n",
    "class Atmos_Data:\n",
    "    day: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    week: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    month: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    season: pd.DataFrame = field(default_factory = pd.DataFrame)\n",
    "    annual: pd.DataFrame = field(default_factory = pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = {}\n",
    "model_date = \"2025-07-10\"\n",
    "site_mapping_coords = {\n",
    "    \"se_st1\": coordinates(68.35415, 19.050333),\n",
    "    \"se_sto\": coordinates(68.35594288, 19.04520892),\n",
    "    \"fi_ken\": coordinates(67.98721, 24.24301),\n",
    "    \"fi_sod\": coordinates(67.36239, 26.63859),\n",
    "    \"fi_var\": coordinates(67.7549, 29.61),\n",
    "}\n",
    "site_mapping_elevation = {\n",
    "    \"se_st1\": 351,\n",
    "    \"se_sto\": 351,\n",
    "    \"fi_ken\": 347,\n",
    "    \"fi_sod\": 187,\n",
    "    \"fi_var\": 395,\n",
    "}\n",
    "\n",
    "for file in os.listdir(\"../arctic-ecosystems/gppe-model-predictions/\"):\n",
    "    site = file[0:6]\n",
    "    date = file[22:32]\n",
    "    \n",
    "    if date == model_date:\n",
    "        print(site, date)\n",
    "        \n",
    "        coords = site_mapping_coords.get(site)\n",
    "        elevation = site_mapping_elevation.get(site)\n",
    "        \n",
    "        sites[site] = Site_Data(\n",
    "            sat_day = pd.read_csv(f\"../arctic-ecosystems/gppe-model-predictions/{site}_satellite-gppe_{date}.csv\"),\n",
    "            cli_day = pd.read_csv(f\"../data/processed/{site}_climate.csv\"),\n",
    "            coords = coords,\n",
    "            elevation = elevation\n",
    "            \n",
    "        )\n",
    "\n",
    "atmos = Atmos_Data(\n",
    "    day = pd.read_csv(\"../data/processed/atmos.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganise to datetime indices\n",
    "for key, item in sites.items():\n",
    "    item.cli_day[\"date\"] = pd.to_datetime(item.cli_day[\"Unnamed: 0\"]).dt.date\n",
    "    item.sat_day[\"date\"] = pd.to_datetime(item.sat_day[\"Unnamed: 0\"]).dt.date\n",
    "    \n",
    "    item.cli_day.set_index(\"date\", inplace = True)\n",
    "    item.sat_day.set_index(\"date\", inplace = True)\n",
    "    \n",
    "    item.cli_day.rename_axis(None, inplace = True)\n",
    "    item.sat_day.rename_axis(None, inplace = True)\n",
    "    \n",
    "    item.sat_day[\"datetime\"] = pd.to_datetime(item.sat_day[\"Unnamed: 0\"])\n",
    "    item.sat_day.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "    item.cli_day.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "    \n",
    "    item.cli_day.index = pd.to_datetime(item.cli_day.index)\n",
    "    item.sat_day.index = pd.to_datetime(item.sat_day.index)\n",
    "\n",
    "atmos.day[\"date\"] = pd.to_datetime(atmos.day[\"Unnamed: 0\"]).dt.date\n",
    "atmos.day.set_index(\"date\", inplace = True)\n",
    "atmos.day.rename_axis(None, inplace = True)\n",
    "atmos.day.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "atmos.day.index = pd.to_datetime(atmos.day.index)\n",
    "\n",
    "\n",
    "# Extract satellite scene platforms into new, unfiltered dict per site or plotting data density later on.\n",
    "density = {}\n",
    "for key, item in sites.items():\n",
    "    density[key] = item.sat_day[[\"platform_id\"]].copy()\n",
    "\n",
    "for key, item in sites.items():\n",
    "    \n",
    "    # Remove Pre 2000 values and post 2025 values from satellite data\n",
    "    item.sat_day = item.sat_day[item.sat_day.index > pd.Timestamp(\"2000-01-01\")]\n",
    "    item.sat_day = item.sat_day[item.sat_day.index < pd.Timestamp(\"2025-01-01\")]\n",
    "\n",
    "    # Take only cols I want\n",
    "    item.sat_day = item.sat_day[[\"gppe\", \"ndvi\", \"gpp\", \"platform_id\", \"system:index\"]]\n",
    "    \n",
    "    # Convert gppe from co2 to C\n",
    "    item.sat_day[\"gppe\"] = item.sat_day[\"gppe\"] * 0.273\n",
    "    item.sat_day[\"gpp\"] = item.sat_day[\"gpp\"] * 0.273\n",
    "    \n",
    "\n",
    "# Remove incorrect kenttarova value\n",
    "sites[\"fi_ken\"].cli_day = sites[\"fi_ken\"].cli_day[sites[\"fi_ken\"].cli_day.index > pd.Timestamp(\"2003-01-01\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Aggregation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation Functions\n",
    "agg_funcs = {\n",
    "    \"mean\": \"mean\",\n",
    "    \"min\": \"min\",\n",
    "    # \"median\": \"median\",\n",
    "    \"max\": \"max\",\n",
    "    \"q05\": lambda x: x.quantile(0.05),\n",
    "    \"q95\": lambda x: x.quantile(0.95),\n",
    "    \"std\": \"std\"\n",
    "}\n",
    "\n",
    "# AUC\n",
    "def calc_auc(group, n_bootstrap = 1000, random_seed = 2):\n",
    "    \n",
    "    group = group.dropna()\n",
    "    if len(group) < 3:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    days_since_start = (group.index - group.index.min()) / np.timedelta64(1, 'D')\n",
    "    days_since_start = days_since_start.values\n",
    "    \n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    bootstrapped_aucs = []\n",
    "    valid_indices = np.arange(len(group))\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample_indices = rng.choice(valid_indices, size=len(valid_indices), replace=True)\n",
    "        sample_x = days_since_start[sample_indices]\n",
    "        sample_y = group.values[sample_indices]\n",
    "        sorted_order = np.argsort(sample_x)\n",
    "        sample_x_sorted = sample_x[sorted_order]\n",
    "        sample_y_sorted = sample_y[sorted_order]\n",
    "        auc_sample = np.trapezoid(y=sample_y_sorted, x=sample_x_sorted)\n",
    "        bootstrapped_aucs.append(auc_sample)\n",
    "\n",
    "    bootstrapped_aucs = np.array(bootstrapped_aucs)\n",
    "    return bootstrapped_aucs.mean(), bootstrapped_aucs.std()\n",
    "\n",
    "# End-of-ablation-season\n",
    "def find_end_of_ablation_season(group, variable):\n",
    "    weekly_means = group[variable].resample(\"W\").mean()\n",
    "    zero_weeks = weekly_means[weekly_means == 0]\n",
    "    \n",
    "    if not zero_weeks.empty:\n",
    "        return zero_weeks.index[0].day_of_year\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# Warmth Index\n",
    "def find_warmth_index(t, min_months, period_mean = \"MS\"):\n",
    "    \n",
    "    t_mean = t.resample(period_mean).mean()\n",
    "    \n",
    "    if t_mean.count() < min_months:\n",
    "        return np.nan\n",
    "    else:\n",
    "        t0 = t_mean[t_mean > 0]\n",
    "        wi = t0.sum()\n",
    "        return wi\n",
    "\n",
    "# Growing Degree Days\n",
    "def find_gdd(t):\n",
    "    # https://www.nature.com/articles/s41597-023-01959-w\n",
    "    \n",
    "    t = t[t > 5] # all temps above 5\n",
    "    if t.empty:\n",
    "        return np.nan\n",
    "    \n",
    "    gdd = t.sum()\n",
    "    \n",
    "    return gdd\n",
    "\n",
    "# Snow Gain and Loss\n",
    "def find_snow_gain(s):\n",
    "    diff = s.diff()\n",
    "    diff_filter = diff[diff > 0]\n",
    "    return diff_filter.sum()\n",
    "def find_snow_loss(s):\n",
    "    diff = s.diff()\n",
    "    diff_filter = diff[diff < 0]\n",
    "    return diff_filter.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aggregates(df, resample_length, is_climate = False, is_satellite = False):\n",
    "    \n",
    "    df = df.copy().select_dtypes(include = np.number)\n",
    "    \n",
    "    # Make appropriate resampled objects\n",
    "    if resample_length == \"week\":\n",
    "        df_resampled = df.resample(\"W\")\n",
    "    elif resample_length == \"month\":\n",
    "        df_resampled = df.resample(\"MS\")\n",
    "    elif resample_length == \"season\":\n",
    "        df_resampled = df.resample(\"QS-DEC\") # pandas is setup for fiscal quarters starting on jan. We need meteorological seasons/quarters starting on Dec.\n",
    "    elif resample_length == \"year\":\n",
    "        if is_climate == True:\n",
    "            df_snow = df[[\"snowdepth\"]]\n",
    "            df_resampled_snow = df_snow.resample(\"YS-AUG\")\n",
    "        \n",
    "        df = df.drop(\"snowdepth\", axis = 1, errors = \"ignore\")\n",
    "        df_resampled = df.resample(\"YS\")\n",
    "            \n",
    "    # Generic Aggregations (on everything but yearly snow)\n",
    "    stats = df_resampled.agg(list(agg_funcs.values()))\n",
    "    stats.columns = pd.MultiIndex.from_product([df.columns, list(agg_funcs.keys())]) # rename aggregations columns\n",
    "    \n",
    "    # Climate Sums (on all climate data but yearly snow)\n",
    "    if is_climate and resample_length in [\"week\", \"month\", \"season\"]:\n",
    "        stats[(\"precipitation\", \"sum\")] = df_resampled[[\"precipitation\"]].sum(min_count = 1)\n",
    "        stats[(\"temperature\", \"gdd\")] = df_resampled[[\"temperature\"]].apply(lambda group: find_gdd(t = group))\n",
    "        stats[(\"snowdepth\", \"gain\")] = df_resampled[[\"snowdepth\"]].apply(lambda group: find_snow_gain(s = group))\n",
    "        stats[(\"snowdepth\", \"loss\")] = df_resampled[[\"snowdepth\"]].apply(lambda group: find_snow_loss(s = group))\n",
    "    \n",
    "    # Satellite AUCs (on just satellite data)\n",
    "    if is_satellite:\n",
    "        auc_gppe = df_resampled[\"gppe\"].apply(calc_auc).apply(pd.Series)\n",
    "        auc_gppe.columns = pd.MultiIndex.from_product([[\"gppe\"], [\"auc\", \"auc_std\"]])\n",
    "        auc_gpp = df_resampled[\"gpp\"].apply(calc_auc).apply(pd.Series)\n",
    "        auc_gpp.columns = pd.MultiIndex.from_product([[\"gpp\"], [\"auc\", \"auc_std\"]])\n",
    "        auc_ndvi = df_resampled[\"ndvi\"].apply(calc_auc).apply(pd.Series)\n",
    "        auc_ndvi.columns = pd.MultiIndex.from_product([[\"ndvi\"], [\"auc\", \"auc_std\"]])\n",
    "        stats = pd.concat([stats, auc_gppe, auc_gpp, auc_ndvi], axis = 1)\n",
    "    \n",
    "    # Warmth index (only seasonally and yearly)\n",
    "    if resample_length in [\"season\", \"year\"] and is_climate == True:\n",
    "        stats_wi = pd.DataFrame()\n",
    "        stats_wi[(\"temperature\", \"wi\")] = df_resampled[(\"temperature\")].apply(lambda group: find_warmth_index(group, min_months = 3, period_mean = \"MS\"))\n",
    "        stats = pd.concat([stats, stats_wi], axis = 1)\n",
    "    \n",
    "    # For yearly snow data\n",
    "    if resample_length == \"year\" and is_climate == True:\n",
    "        \n",
    "        # Generic aggregations\n",
    "        stats_snow = df_resampled_snow.agg(list(agg_funcs.values()))\n",
    "        stats_snow.columns = pd.MultiIndex.from_product([df_snow.columns, list(agg_funcs.keys())]) # rename aggregations columns\n",
    "        \n",
    "        # Gain and loss\n",
    "        stats_snow[(\"snowdepth\", \"gain\")] = df_resampled_snow[[\"snowdepth\"]].apply(lambda group: find_snow_gain(s = group))\n",
    "        stats_snow[(\"snowdepth\", \"loss\")] = df_resampled_snow[[\"snowdepth\"]].apply(lambda group: find_snow_loss(s = group))\n",
    "        \n",
    "        # EOAS\n",
    "        stats_snow[(\"snowdepth\", \"end-of-ablation-season\")] = df_resampled_snow.apply(lambda group: find_end_of_ablation_season(group, \"snowdepth\"))\n",
    "        stats_snow = stats_snow.shift(1, \"YS\") # by calculating on a resample starting mid-year (df_resampled_snow), the snow aggregates use the year of their beggining date which comes from the previous year. We shift it back to the current year.\n",
    "        stats = pd.concat([stats, stats_snow], axis = 1)\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmos.week = calc_aggregates(df = atmos.day, resample_length = \"week\")\n",
    "atmos.month = calc_aggregates(df = atmos.day, resample_length = \"month\")\n",
    "atmos.season = calc_aggregates(df = atmos.day, resample_length = \"season\")\n",
    "atmos.annual = calc_aggregates(df = atmos.day, resample_length = \"year\")\n",
    "\n",
    "for site, item in sites.items():\n",
    "    item.sat_week = calc_aggregates(df = item.sat_day, resample_length = \"week\", is_satellite = True)\n",
    "    item.sat_month = calc_aggregates(df = item.sat_day, resample_length = \"month\", is_satellite = True)\n",
    "    item.sat_season = calc_aggregates(df = item.sat_day, resample_length = \"season\", is_satellite = True)\n",
    "    item.sat_annual = calc_aggregates(df = item.sat_day, resample_length = \"year\", is_satellite = True)\n",
    "    \n",
    "for site, item in sites.items():\n",
    "    item.cli_week = calc_aggregates(df = item.cli_day, resample_length = \"week\", is_climate = True)\n",
    "    item.cli_month = calc_aggregates(df = item.cli_day, resample_length = \"month\", is_climate = True)\n",
    "    item.cli_season = calc_aggregates(df = item.cli_day, resample_length = \"season\", is_climate = True)\n",
    "    item.cli_annual = calc_aggregates(df = item.cli_day, resample_length = \"year\", is_climate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"pkl/analysis_ready_sites.pkl\", 'wb') as f:\n",
    "        pickle.dump(sites, f)\n",
    "with open(f\"pkl/analysis_ready_atmos.pkl\", 'wb') as f:\n",
    "        pickle.dump(atmos, f)\n",
    "with open(f\"pkl/density.pkl\", 'wb') as f:\n",
    "        pickle.dump(density, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pickle Data\n",
    "sites = None\n",
    "with open(\"pkl/analysis_ready_sites.pkl\", 'rb') as f:\n",
    "    sites = pickle.load(f)\n",
    "atmos = None\n",
    "with open(\"pkl/analysis_ready_atmos.pkl\", 'rb') as f:\n",
    "    atmos = pickle.load(f)\n",
    "density = None\n",
    "with open(\"pkl/density.pkl\", 'rb') as f:\n",
    "    density = pickle.load(f)\n",
    "model_stats = None\n",
    "with open(\"pkl/model_stats.pkl\", 'rb') as f:\n",
    "    model_stats = pickle.load(f)\n",
    "\n",
    "\n",
    "# Order the sites in the dictionaries\n",
    "order = [\"se_st1\", \"se_sto\", \"fi_ken\", \"fi_sod\", \"fi_var\"]\n",
    "res = dict()\n",
    "for key in order:\n",
    "    res[key] = sites[key]\n",
    "sites = res\n",
    "\n",
    "order = [\"se_st1\", \"se_sto\", \"fi_ken\", \"fi_sod\", \"fi_var\"]\n",
    "res = dict()\n",
    "for key in order:\n",
    "    res[key] = density[key]\n",
    "density = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dictionary to contain statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = {}\n",
    "for key in sites:\n",
    "    stats_dict[key] = {\n",
    "        \"coords\": sites[key].coords,\n",
    "        \"elevation\": sites[key].elevation\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set global value for the number of bootstrap iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_n_boots = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime to Decimal Years\n",
    "def datetime_to_decimal_years(datetime_index):\n",
    "    return datetime_index.year + (datetime_index.dayofyear - 1) / 365.25\n",
    "\n",
    "\n",
    "# OLS Regression\n",
    "def ols(x, y, n_boot_samples = False, random_state = 2):\n",
    "\n",
    "    # OLS\n",
    "    results = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "    x_range = np.linspace(x.min(), x.max(), 100)\n",
    "    y_pred = results.predict(sm.add_constant(x_range))\n",
    "    \n",
    "    # Bootstrap CIs\n",
    "    if n_boot_samples != False:\n",
    "        rng = np.random.default_rng(seed = random_state)\n",
    "        n = len(x)\n",
    "        boot_preds = []\n",
    "        boot_resids = []\n",
    "        \n",
    "        for _ in range(n_boot_samples):\n",
    "            \n",
    "            # Resample\n",
    "            idx = rng.choice(n, size = n, replace = True)\n",
    "            x_boot, y_boot = x.iloc[idx], y.iloc[idx]\n",
    "\n",
    "            # Fit & Predict on idealised Xs (for smooth regression)\n",
    "            results_boot = sm.OLS(y_boot, sm.add_constant(x_boot)).fit() # remember to use a different named variable inside the bootstrap loop\n",
    "            y_pred_boot = results_boot.predict(sm.add_constant(x_range)) # use the same prediction range as above. Important! This makes all boot predictions use the same x range.\n",
    "            \n",
    "            # Fit & Predict on real Xs (for residual error bars)\n",
    "            y_pred_boot_orginal = results_boot.predict(sm.add_constant(x)) # predict again but on original Xs so we can find y datapoints residuals. These are the actual y values of where the error is, not a relative error\n",
    "            y_resid_boot = y - y_pred_boot_orginal\n",
    "            \n",
    "            # Store boot iterations\n",
    "            boot_preds.append(y_pred_boot) # append pd.series to list\n",
    "            boot_resids.append(y_resid_boot)\n",
    "            \n",
    "        # Collapse boot iterations to arrays\n",
    "        boot_preds = np.array(boot_preds) # collapse list to np.array (converting from pd.series to np.array)\n",
    "        boot_resids = pd.concat(boot_resids, axis = 1) # collapse to a df for this one so we can keep the index of the pd.series for merging/calcs with the original residuals later. Also doing sort false as apparently pandas likes to auto sort during a concat and our index is on sortable because dupicates (?). Very annoying, adding sort = false suppresses the warning.   sort = False\n",
    "        \n",
    "        # Y-pred Percentiles\n",
    "        ci_lo = np.percentile(boot_preds, 5, axis = 0)\n",
    "        ci_hi = np.percentile(boot_preds, 95, axis = 0)\n",
    "        \n",
    "        # Y-resid Percentiles\n",
    "        resid_lo = boot_resids.quantile(0.05, axis = 1) # use quantile here as we are not working with np array but a pd.dataframe\n",
    "        resid_hi = boot_resids.quantile(0.95, axis = 1)\n",
    "        #resid_se = boot_resids.std(axis = 1) # not the right stat to use as we are looking at model uncertainty, not sample variation (???)\n",
    "        \n",
    "        # Return\n",
    "        OLS_Stats = namedtuple(\"OLS_Stats\", [\"results\", \"x_range\", \"y_pred\", \"ci_lo\", \"ci_hi\", \"resid_lo\", \"resid_hi\"])#, \"resid_se\"])\n",
    "        return OLS_Stats(results, x_range, y_pred, ci_lo, ci_hi, resid_lo, resid_hi)#, resid_se)\n",
    "    else: \n",
    "        OLS_Stats = namedtuple(\"OLS_Stats\", [\"results\", \"x_range\", \"y_pred\"])\n",
    "        return OLS_Stats(results, x_range, y_pred)\n",
    "\n",
    "# Theil-sen\n",
    "def theilsen(x, y, n_boot_samples = False, random_state = 2):\n",
    "    \n",
    "    # Theil-sen\n",
    "    slope, intercept, slope_lo , slope_hi = theilslopes(y.copy(), x = x.copy(), alpha = 0.95, method = \"separate\") # using copy as it sorts the data\n",
    "    x_range = np.linspace(x.min(), x.max(), 100)\n",
    "    y_pred = intercept + slope * x_range\n",
    "    \n",
    "    # Bootstrap CIs\n",
    "    if n_boot_samples != False:\n",
    "        rng = np.random.default_rng(seed = random_state)\n",
    "        n = len(x)\n",
    "        boot_preds = []\n",
    "\n",
    "        for _ in range(n_boot_samples):\n",
    "            \n",
    "            # Resample\n",
    "            idx = rng.choice(n, size = n, replace = True)\n",
    "            x_boot, y_boot = x.iloc[idx], y.iloc[idx]\n",
    "\n",
    "            # Fit & Predict\n",
    "            slope, intercept, *_  = theilslopes(y_boot.copy(), x = x_boot.copy(), alpha = 0.95, method = \"separate\") # using copy as it sorts the data\n",
    "            y_pred_boot = intercept + slope * x_range # use the same prediction range as above. Important! This makes all boot predictions use the same x range.\n",
    "            \n",
    "            boot_preds.append(y_pred_boot)\n",
    "\n",
    "        boot_preds = np.array(boot_preds)\n",
    "\n",
    "        # Percentiles\n",
    "        ci_lo = np.percentile(boot_preds, 10, axis = 0)\n",
    "        ci_hi = np.percentile(boot_preds, 90, axis = 0)\n",
    "        \n",
    "        # Return\n",
    "        TheilSen_Stats = namedtuple(\"TheilSen_Stats\", [\"slope\", \"intercept\", \"slope_lo\", \"slope_hi\", \"y_pred\", \"x_range\", \"ci_lo\", \"ci_hi\"])\n",
    "        return TheilSen_Stats(slope, intercept, slope_lo, slope_hi , y_pred, x_range, ci_lo, ci_hi)\n",
    "    else: \n",
    "        TheilSen_Stats = namedtuple(\"TheilSen_Stats\", [\"slope\", \"intercept\", \"slope_lo\", \"slope_hi\", \"y_pred\", \"x_range\"])\n",
    "        return TheilSen_Stats(slope, intercept, slope_lo, slope_hi , y_pred, x_range)\n",
    "    \n",
    "# Kendall Trend Test\n",
    "def kendall_trend(x, y):\n",
    "    kt_tau, kt_pval = kendalltau(x, y)\n",
    "    KT_Stats = namedtuple(\"KT_Stats\", [\"tau\", \"pval\"])\n",
    "    return KT_Stats(kt_tau, kt_pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No. Unique Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([sites[site].sat_day.copy() for site in sites], axis = 0)\n",
    "print(f\"Number of Unique Quality Satellite Scenes: {df[\"system:index\"].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPPe Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"index\", \"\")\n",
    "y_col = (\"gppe\", \"auc\")\n",
    "y_col_std = (\"gppe\", \"auc_std\")\n",
    "var_names = [\"index\", \"gppe\", \"std\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = item.sat_annual.copy()\n",
    "    df.index = datetime_to_decimal_years(df.index)\n",
    "    df = df.reset_index()\n",
    "    df = df[[x_col, y_col, y_col_std]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "\n",
    "    # Stats\n",
    "    sample_std = np.std(df[y_var], axis = 0)\n",
    "    ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "    ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    \n",
    "    # Add entry to stats dict\n",
    "    stats_dict[key][\"gppe-time_annual\"] = {\n",
    "        \"df\": df,\n",
    "        \"x_var\": var_names[0],\n",
    "        \"y_var\": var_names[1],\n",
    "        \"y_var_std\": var_names[2],\n",
    "        \"sample_std\": sample_std,\n",
    "        \"ols\": ols_stats,\n",
    "        \"kt\": kt_stats,\n",
    "        \"ts\": ts_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (key, item) in enumerate(stats_dict.items()):\n",
    "    \n",
    "    item = item[\"gppe-time_annual\"]\n",
    "    #ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "    #ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "    ts_slope = item[\"ts\"].slope\n",
    "    kt_tau = item[\"kt\"].tau\n",
    "    kt_pval = item[\"kt\"].pval\n",
    "    \n",
    "    print(f\"At {key}, {item[\"y_var\"]} over {item[\"x_var\"]} reported {ts_slope:.2f} gCm⁻²a⁻¹ (τ:{kt_tau:.2f}, p:{kt_pval:.2f}). Sample StdDev: ±{item['sample_std'].item():.2f} (1σ).\")\n",
    "    \n",
    "    \n",
    "    # Check Data Distribution\n",
    "    data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "    # Fit a normal distribution\n",
    "    mu, std = norm.fit(data)\n",
    "    # Generate x values for normal CDF\n",
    "    x = np.linspace(data.min(), data.max(), 500)\n",
    "    normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (12, 4), layout = \"constrained\")\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    # CDF\n",
    "    ax[0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "    ax[0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "    ax[0].set_title(\"Normal Dist. CDF\")\n",
    "    \n",
    "    # Data\n",
    "    ax[1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "    ax[1].plot(item[\"ts\"].x_range, item[\"ts\"].y_pred, color = \"C0\")\n",
    "    ax[1].fill_between(\n",
    "        item[\"ts\"].x_range.flatten(),\n",
    "        np.array(item[\"ts\"].ci_lo).flatten(),\n",
    "        np.array(item[\"ts\"].ci_hi).flatten(),\n",
    "        color = \"black\",\n",
    "        alpha = 0.1\n",
    "    )\n",
    "    ax[1].errorbar(\n",
    "        item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "        yerr = item[\"df\"][item[\"y_var_std\"]].squeeze(),\n",
    "        fmt = \"o\",\n",
    "        elinewidth = 0.5,\n",
    "        capsize = 2.5,\n",
    "        color = \"C0\",\n",
    "        alpha = 1\n",
    "    )\n",
    "    ax[1].set_title(key)\n",
    "    ax[1].set_xlabel(item[\"x_var\"])\n",
    "    ax[1].set_ylabel(item[\"y_var\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"index\", \"\")\n",
    "y_col = (\"gppe\", \"auc\")\n",
    "y_col_std = (\"gppe\", \"auc_std\")\n",
    "var_names = [\"index\", \"gppe\", \"std\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = sites[key].sat_season.copy()\n",
    "    df[x_var] = datetime_to_decimal_years(df.index)\n",
    "    df = df[[x_col, y_col, y_col_std]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "    \n",
    "    seasons = {\n",
    "        \"summer\": df[df.index.month.isin([6, 7, 8])],\n",
    "        #\"winter\": df[df.index.month.isin([12, 1, 2])], # winter has no gpp so no need to do stats or plot it\n",
    "        \"spring\": df[df.index.month.isin([3, 4, 5])],\n",
    "        \"autumn\": df[df.index.month.isin([9, 10, 11])]\n",
    "    }\n",
    "    \n",
    "    stats_dict[key][\"gppe-time_season\"] = {}\n",
    "    for season, df in seasons.items():\n",
    "\n",
    "        # Stats\n",
    "        sample_std = np.std(df[y_var], axis = 0)\n",
    "        ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "        kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "        ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "        \n",
    "        # Add entry to stats dict\n",
    "        stats_dict[key][\"gppe-time_season\"][season] = {\n",
    "            \"df\": df,\n",
    "            \"x_var\": var_names[0],\n",
    "            \"y_var\": var_names[1],\n",
    "            \"y_var_std\": var_names[2],\n",
    "            \"sample_std\": sample_std,\n",
    "            \"ols\": ols_stats,\n",
    "            \"kt\": kt_stats,\n",
    "            \"ts\": ts_stats\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site, entry in stats_dict.items():\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(3, 2, figsize = (12, 4*3), layout = \"constrained\")\n",
    "    \n",
    "    for idx, (season, item) in enumerate(entry[\"gppe-time_season\"].items()):\n",
    "\n",
    "        #ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "        #ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "        ts_slope = item[\"ts\"].slope\n",
    "        n = item[\"y_var_std\"].count\n",
    "        kt_tau = item[\"kt\"].tau\n",
    "        kt_pval = item[\"kt\"].pval\n",
    "            \n",
    "        print(f\"At {key}, {item[\"y_var\"]} over {item[\"x_var\"]} reported {ts_slope:.2f} gCm⁻²a⁻¹ (τ:{kt_tau:.2f}, p:{kt_pval:.2f}). Sample StdDev: ±{item['sample_std'].item():.2f} (1σ).\")\n",
    "        \n",
    "        # Check Data Distribution\n",
    "        data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "        # Fit a normal distribution\n",
    "        mu, std = norm.fit(data)\n",
    "        # Generate x values for normal CDF\n",
    "        x = np.linspace(data.min(), data.max(), 500)\n",
    "        normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "        \n",
    "        # CDF\n",
    "        ax[idx, 0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "        ax[idx, 0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "        ax[idx, 0].set_title(\"Normal Dist. CDF\")\n",
    "        \n",
    "        # Data\n",
    "        ax[idx, 1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "        ax[idx, 1].plot(item[\"ts\"].x_range, item[\"ts\"].y_pred, color = \"C0\")\n",
    "        ax[idx, 1].fill_between(\n",
    "            item[\"ts\"].x_range.flatten(),\n",
    "            np.array(item[\"ts\"].ci_lo).flatten(),\n",
    "            np.array(item[\"ts\"].ci_hi).flatten(),\n",
    "            color = \"black\",\n",
    "            alpha = 0.1\n",
    "        )\n",
    "        ax[idx, 1].errorbar(\n",
    "            item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "            yerr = item[\"df\"][item[\"y_var_std\"]].squeeze(),\n",
    "            fmt = \"o\",\n",
    "            elinewidth = 0.5,\n",
    "            capsize = 2.5,\n",
    "            color = \"C0\",\n",
    "            alpha = 1\n",
    "        )\n",
    "        ax[idx, 1].set_title(key)\n",
    "        ax[idx, 1].set_xlabel(item[\"x_var\"])\n",
    "        ax[idx, 1].set_ylabel(item[\"y_var\"])\n",
    "        ax[idx, 1].set_title(f\"{season}: {item[\"y_var\"]} over {item[\"x_var\"]}\")\n",
    "        if idx == 3:\n",
    "            ax[idx, 1].set_xlabel(item[\"x_var\"])\n",
    "        ax[idx, 1].set_ylabel(item[\"y_var\"])\n",
    "\n",
    "    plt.show()    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPPe & NDVI Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPPe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"index\", \"\")\n",
    "y_col = (\"gppe\", \"auc\")\n",
    "var_names = [\"index\", \"gppe\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = item.sat_annual.copy()\n",
    "    df.index = datetime_to_decimal_years(df.index)\n",
    "    df = df.reset_index()\n",
    "    df = df[[x_col, y_col]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "    \n",
    "    # Z-score\n",
    "    df[y_var] = zscore(df[y_var])\n",
    "\n",
    "    # Stats\n",
    "    sample_std = np.std(df[y_var], axis = 0)\n",
    "    ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "    ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    \n",
    "    # Add entry to stats dict\n",
    "    stats_dict[key][\"gppe_ndvi-time_annual\"] = {}\n",
    "    stats_dict[key][\"gppe_ndvi-time_annual\"][\"gppe\"] = {\n",
    "        \"df\": df,\n",
    "        \"x_var\": var_names[0],\n",
    "        \"y_var\": var_names[1],\n",
    "        \"sample_std\": sample_std,\n",
    "        \"ols\": ols_stats,\n",
    "        \"kt\": kt_stats,\n",
    "        \"ts\": ts_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (key, item) in enumerate(stats_dict.items()):\n",
    "    \n",
    "    item = item[\"gppe_ndvi-time_annual\"][\"gppe\"]\n",
    "    #ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "    #ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "    ts_slope = item[\"ts\"].slope\n",
    "    kt_tau = item[\"kt\"].tau\n",
    "    kt_pval = item[\"kt\"].pval\n",
    "    \n",
    "    print(f\"At {key}, {item[\"y_var\"]} over {item[\"x_var\"]} reported {ts_slope:.2f} Z-score (τ:{kt_tau:.2f}, p:{kt_pval:.2f})\")\n",
    "    \n",
    "    \n",
    "    # Check Data Distribution\n",
    "    data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "    # Fit a normal distribution\n",
    "    mu, std = norm.fit(data)\n",
    "    # Generate x values for normal CDF\n",
    "    x = np.linspace(data.min(), data.max(), 500)\n",
    "    normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (12, 4), layout = \"constrained\")\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    # CDF\n",
    "    ax[0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "    ax[0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "    ax[0].set_title(\"Normal Dist. CDF\")\n",
    "    \n",
    "    # Data\n",
    "    ax[1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "    plt.plot(item[\"ts\"].x_range, item[\"ts\"].y_pred, color = \"C0\")\n",
    "    ax[1].fill_between(\n",
    "        item[\"ts\"].x_range.flatten(),\n",
    "        np.array(item[\"ts\"].ci_lo).flatten(),\n",
    "        np.array(item[\"ts\"].ci_hi).flatten(),\n",
    "        color = \"black\",\n",
    "        alpha = 0.1\n",
    "    )\n",
    "    #ax[1].errorbar(\n",
    "    #    item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "    #    yerr = item[\"df\"][item[\"y_var_std\"]].squeeze(),\n",
    "    #    fmt = \"o\",\n",
    "    #    elinewidth = 0.5,\n",
    "    #    capsize = 2.5,\n",
    "    #    color = \"C0\",\n",
    "    #    alpha = 1\n",
    "    #)\n",
    "    ax[1].set_title(key)\n",
    "    ax[1].set_xlabel(item[\"x_var\"])\n",
    "    ax[1].set_ylabel(item[\"y_var\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"index\", \"\")\n",
    "y_col = (\"ndvi\", \"auc\")\n",
    "var_names = [\"index\", \"ndvi\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = item.sat_annual.copy()\n",
    "    df.index = datetime_to_decimal_years(df.index)\n",
    "    df = df.reset_index()\n",
    "    df = df[[x_col, y_col]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "    \n",
    "    # Z-score\n",
    "    df[y_var] = zscore(df[y_var])\n",
    "\n",
    "    # Stats\n",
    "    sample_std = np.std(df[y_var], axis = 0)\n",
    "    ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "    ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    \n",
    "    # Add entry to stats dict\n",
    "    #stats_dict[key][\"gppe_ndvi-time_annual\"] = {} # already made when calculaitng gppe stats\n",
    "    stats_dict[key][\"gppe_ndvi-time_annual\"][\"ndvi\"] = {\n",
    "        \"df\": df,\n",
    "        \"x_var\": var_names[0],\n",
    "        \"y_var\": var_names[1],\n",
    "        \"sample_std\": sample_std,\n",
    "        \"ols\": ols_stats,\n",
    "        \"kt\": kt_stats,\n",
    "        \"ts\": ts_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (key, item) in enumerate(stats_dict.items()):\n",
    "    \n",
    "    item = item[\"gppe_ndvi-time_annual\"][\"ndvi\"]\n",
    "    #ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "    #ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "    ts_slope = item[\"ts\"].slope\n",
    "    kt_tau = item[\"kt\"].tau\n",
    "    kt_pval = item[\"kt\"].pval\n",
    "    \n",
    "    print(f\"At {key}, {item[\"y_var\"]} over {item[\"x_var\"]} reported {ts_slope:.2f} Z-score (τ:{kt_tau:.2f}, p:{kt_pval:.2f})\")\n",
    "    \n",
    "    \n",
    "    # Check Data Distribution\n",
    "    data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "    # Fit a normal distribution\n",
    "    mu, std = norm.fit(data)\n",
    "    # Generate x values for normal CDF\n",
    "    x = np.linspace(data.min(), data.max(), 500)\n",
    "    normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (12, 4), layout = \"constrained\")\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    # CDF\n",
    "    ax[0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "    ax[0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "    ax[0].set_title(\"Normal Dist. CDF\")\n",
    "    \n",
    "    # Data\n",
    "    ax[1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "    plt.plot(item[\"ts\"].x_range, item[\"ts\"].y_pred, color = \"C0\")\n",
    "    ax[1].fill_between(\n",
    "        item[\"ts\"].x_range.flatten(),\n",
    "        np.array(item[\"ts\"].ci_lo).flatten(),\n",
    "        np.array(item[\"ts\"].ci_hi).flatten(),\n",
    "        color = \"black\",\n",
    "        alpha = 0.1\n",
    "    )\n",
    "    #ax[1].errorbar(\n",
    "    #    item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "    #    yerr = item[\"df\"][item[\"y_var_std\"]].squeeze(),\n",
    "    #    fmt = \"o\",\n",
    "    #    elinewidth = 0.5,\n",
    "    #    capsize = 2.5,\n",
    "    #    color = \"C0\",\n",
    "    #    alpha = 1\n",
    "    #)\n",
    "    ax[1].set_title(key)\n",
    "    ax[1].set_xlabel(item[\"x_var\"])\n",
    "    ax[1].set_ylabel(item[\"y_var\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPPe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"index\", \"\")\n",
    "y_col = (\"gppe\", \"max\")\n",
    "var_names = [\"index\", \"gppe\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = item.sat_annual.copy()\n",
    "    df.index = datetime_to_decimal_years(df.index)\n",
    "    df = df.reset_index()\n",
    "    df = df[[x_col, y_col]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "    \n",
    "    # Z-score\n",
    "    df[y_var] = zscore(df[y_var])\n",
    "\n",
    "    # Stats\n",
    "    sample_std = np.std(df[y_var], axis = 0)\n",
    "    ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "    ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    \n",
    "    # Add entry to stats dict\n",
    "    stats_dict[key][\"gppe_ndvi-time_peak\"] = {} # already made when calculaitng gppe stats\n",
    "    stats_dict[key][\"gppe_ndvi-time_peak\"][\"gppe\"] = {\n",
    "        \"df\": df,\n",
    "        \"x_var\": var_names[0],\n",
    "        \"y_var\": var_names[1],\n",
    "        \"sample_std\": sample_std,\n",
    "        \"ols\": ols_stats,\n",
    "        \"kt\": kt_stats,\n",
    "        \"ts\": ts_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (key, item) in enumerate(stats_dict.items()):\n",
    "    \n",
    "    item = item[\"gppe_ndvi-time_peak\"][\"gppe\"]\n",
    "    #ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "    #ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "    ts_slope = item[\"ts\"].slope\n",
    "    kt_tau = item[\"kt\"].tau\n",
    "    kt_pval = item[\"kt\"].pval\n",
    "    \n",
    "    print(f\"At {key}, {item[\"y_var\"]} over {item[\"x_var\"]} reported {ts_slope:.2f} Z-score (τ:{kt_tau:.2f}, p:{kt_pval:.2f})\")\n",
    "    \n",
    "    \n",
    "    # Check Data Distribution\n",
    "    data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "    # Fit a normal distribution\n",
    "    mu, std = norm.fit(data)\n",
    "    # Generate x values for normal CDF\n",
    "    x = np.linspace(data.min(), data.max(), 500)\n",
    "    normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (12, 4), layout = \"constrained\")\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    # CDF\n",
    "    ax[0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "    ax[0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "    ax[0].set_title(\"Normal Dist. CDF\")\n",
    "    \n",
    "    # Data\n",
    "    ax[1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "    plt.plot(item[\"ts\"].x_range, item[\"ts\"].y_pred, color = \"C0\")\n",
    "    ax[1].fill_between(\n",
    "        item[\"ts\"].x_range.flatten(),\n",
    "        np.array(item[\"ts\"].ci_lo).flatten(),\n",
    "        np.array(item[\"ts\"].ci_hi).flatten(),\n",
    "        color = \"black\",\n",
    "        alpha = 0.1\n",
    "    )\n",
    "    #ax[1].errorbar(\n",
    "    #    item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "    #    yerr = item[\"df\"][item[\"y_var_std\"]].squeeze(),\n",
    "    #    fmt = \"o\",\n",
    "    #    elinewidth = 0.5,\n",
    "    #    capsize = 2.5,\n",
    "    #    color = \"C0\",\n",
    "    #    alpha = 1\n",
    "    #)\n",
    "    ax[1].set_title(key)\n",
    "    ax[1].set_xlabel(item[\"x_var\"])\n",
    "    ax[1].set_ylabel(item[\"y_var\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"index\", \"\")\n",
    "y_col = (\"ndvi\", \"max\")\n",
    "var_names = [\"index\", \"ndvi\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = item.sat_annual.copy()\n",
    "    df.index = datetime_to_decimal_years(df.index)\n",
    "    df = df.reset_index()\n",
    "    df = df[[x_col, y_col]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "    \n",
    "    # Z-score\n",
    "    df[y_var] = zscore(df[y_var])\n",
    "\n",
    "    # Stats\n",
    "    sample_std = np.std(df[y_var], axis = 0)\n",
    "    ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "    ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    \n",
    "    # Add entry to stats dict\n",
    "    #stats_dict[key][\"gppe_ndvi-time_peak\"] = {} # already made when calculaitng gppe stats\n",
    "    stats_dict[key][\"gppe_ndvi-time_peak\"][\"ndvi\"] = {\n",
    "        \"df\": df,\n",
    "        \"x_var\": var_names[0],\n",
    "        \"y_var\": var_names[1],\n",
    "        \"sample_std\": sample_std,\n",
    "        \"ols\": ols_stats,\n",
    "        \"kt\": kt_stats,\n",
    "        \"ts\": ts_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (key, item) in enumerate(stats_dict.items()):\n",
    "    \n",
    "    item = item[\"gppe_ndvi-time_peak\"][\"ndvi\"]\n",
    "    #ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "    #ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "    ts_slope = item[\"ts\"].slope\n",
    "    kt_tau = item[\"kt\"].tau\n",
    "    kt_pval = item[\"kt\"].pval\n",
    "    \n",
    "    print(f\"At {key}, {item[\"y_var\"]} over {item[\"x_var\"]} reported {ts_slope:.2f} Z-score (τ:{kt_tau:.2f}, p:{kt_pval:.2f})\")\n",
    "    \n",
    "    \n",
    "    # Check Data Distribution\n",
    "    data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "    # Fit a normal distribution\n",
    "    mu, std = norm.fit(data)\n",
    "    # Generate x values for normal CDF\n",
    "    x = np.linspace(data.min(), data.max(), 500)\n",
    "    normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (12, 4), layout = \"constrained\")\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    # CDF\n",
    "    ax[0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "    ax[0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "    ax[0].set_title(\"Normal Dist. CDF\")\n",
    "    \n",
    "    # Data\n",
    "    ax[1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "    plt.plot(item[\"ts\"].x_range, item[\"ts\"].y_pred, color = \"C0\")\n",
    "    ax[1].fill_between(\n",
    "        item[\"ts\"].x_range.flatten(),\n",
    "        np.array(item[\"ts\"].ci_lo).flatten(),\n",
    "        np.array(item[\"ts\"].ci_hi).flatten(),\n",
    "        color = \"black\",\n",
    "        alpha = 0.1\n",
    "    )\n",
    "    #ax[1].errorbar(\n",
    "    #    item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "    #    yerr = item[\"df\"][item[\"y_var_std\"]].squeeze(),\n",
    "    #    fmt = \"o\",\n",
    "    #    elinewidth = 0.5,\n",
    "    #    capsize = 2.5,\n",
    "    #    color = \"C0\",\n",
    "    #    alpha = 1\n",
    "    #)\n",
    "    ax[1].set_title(key)\n",
    "    ax[1].set_xlabel(item[\"x_var\"])\n",
    "    ax[1].set_ylabel(item[\"y_var\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPPe and Temperature (WI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"temperature\", \"wi\")\n",
    "y_col = (\"gppe\", \"auc\")\n",
    "y_col_std = (\"gppe\", \"auc_std\")\n",
    "var_names = [\"temperature\", \"gppe\", \"std\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = pd.merge(left = item.sat_annual.copy(), right = item.cli_annual.copy(), left_index = True, right_index = True, how = \"inner\")\n",
    "    #df.index = datetime_to_decimal_years(df.index)\n",
    "    #df = df.reset_index()\n",
    "    df = df[[x_col, y_col, y_col_std]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "\n",
    "    # Stats\n",
    "    sample_std = np.std(df[y_var], axis = 0)\n",
    "    ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    #kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "    #ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    \n",
    "    # Add entry to stats dict\n",
    "    stats_dict[key][\"gppe-wi_annual\"] = {\n",
    "        \"df\": df,\n",
    "        \"x_var\": var_names[0],\n",
    "        \"y_var\": var_names[1],\n",
    "        \"y_var_std\": var_names[2],\n",
    "        \"sample_std\": sample_std,\n",
    "        \"ols\": ols_stats,\n",
    "        #\"kt\": kt_stats,\n",
    "        #\"ts\": ts_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (key, item) in enumerate(stats_dict.items()):\n",
    "    \n",
    "    item = item[\"gppe-wi_annual\"]\n",
    "    ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "    ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "    #ts_slope = item[\"ts\"].slope\n",
    "    #kt_tau = item[\"kt\"].tau\n",
    "    #kt_pval = item[\"kt\"].pval\n",
    "    \n",
    "    print(f\"At {key}, {item[\"y_var\"]} over {item[\"x_var\"]} reported (adj.r2:{ols_r2:.2f}, p:{ols_pval:.2f})\")\n",
    "     \n",
    "    print(item[\"ols\"].results.summary())\n",
    "    \n",
    "    # Check Data Distribution\n",
    "    data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "    # Fit a normal distribution\n",
    "    mu, std = norm.fit(data)\n",
    "    # Generate x values for normal CDF\n",
    "    x = np.linspace(data.min(), data.max(), 500)\n",
    "    normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (12, 4), layout = \"constrained\")\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    # CDF\n",
    "    ax[0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "    ax[0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "    ax[0].set_title(\"Normal Dist. CDF\")\n",
    "    \n",
    "    # Data\n",
    "    ax[1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "    ax[1].plot(item[\"ols\"].x_range, item[\"ols\"].results.predict(sm.add_constant(item[\"ols\"].x_range)))\n",
    "    ax[1].fill_between(item[\"ols\"].x_range.flatten(), item[\"ols\"].ci_lo, item[\"ols\"].ci_hi, alpha = 0.1, color = \"black\", lw = 0)\n",
    "    ax[1].errorbar(\n",
    "        item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "        yerr = item[\"df\"][item[\"y_var_std\"]].squeeze(),\n",
    "        fmt = \"o\",\n",
    "        elinewidth = 0.5,\n",
    "        capsize = 2.5,\n",
    "        color = \"C0\",\n",
    "        alpha = 1\n",
    "    )\n",
    "    ax[1].set_title(key)\n",
    "    ax[1].set_xlabel(item[\"x_var\"])\n",
    "    ax[1].set_ylabel(item[\"y_var\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"temperature\", \"wi\")\n",
    "y_col = (\"gppe\", \"auc\")\n",
    "y_col_std = (\"gppe\", \"auc_std\")\n",
    "var_names = [\"temperature\", \"gppe\", \"std\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = pd.merge(left = item.sat_season.copy(), right = item.cli_season.copy(), left_index = True, right_index = True, how = \"inner\")\n",
    "    #df.index = datetime_to_decimal_years(df.index)\n",
    "    #df = df.reset_index()\n",
    "    df = df[[x_col, y_col, y_col_std]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "    y_var_std = var_names[2]\n",
    "    \n",
    "    seasons = {\n",
    "        \"summer\": df[df.index.month.isin([6, 7, 8])],\n",
    "        #\"winter\": df[df.index.month.isin([12, 1, 2])], # no gppe in winter!\n",
    "        \"spring\": df[df.index.month.isin([3, 4, 5])],\n",
    "        \"autumn\": df[df.index.month.isin([9, 10, 11])]\n",
    "    }\n",
    "    \n",
    "    stats_dict[key][\"gppe-wi_season\"] = {}\n",
    "    for season, df in seasons.items():\n",
    "\n",
    "        # Stats\n",
    "        sample_std = np.std(df[y_var], axis = 0)\n",
    "        ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "        #kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "        #ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "        \n",
    "        # Add entry to stats dict\n",
    "        stats_dict[key][\"gppe-wi_season\"][season] = {\n",
    "            \"df\": df,\n",
    "            \"x_var\": var_names[0],\n",
    "            \"y_var\": var_names[1],\n",
    "            \"y_var_std\": var_names[2],\n",
    "            \"sample_std\": sample_std,\n",
    "            \"ols\": ols_stats,\n",
    "            #\"kt\": kt_stats,\n",
    "            #\"ts\": ts_stats\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site, entry in stats_dict.items():\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(4, 2, figsize = (12, 4*4), layout = \"constrained\")\n",
    "    \n",
    "    for idx, (season, item) in enumerate(entry[\"gppe-wi_season\"].items()):\n",
    "        \n",
    "        ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "        ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "        #ts_slope = item[\"ts\"].slope\n",
    "        #n = item[\"y_std\"].count\n",
    "        #kt_tau = item[\"kt\"].tau\n",
    "        #kt_pval = item[\"kt\"].pval\n",
    "            \n",
    "        print(f\"At {site} in {season}, {item[\"y_var\"]} over {item[\"x_var\"]} reported (adj.r2:{ols_r2:.2f}, p:{ols_pval:.2f})\")\n",
    "        \n",
    "        print(item[\"ols\"].results.summary())\n",
    "        print(\"\")\n",
    "        \n",
    "        # Check Data Distribution\n",
    "        data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "        # Fit a normal distribution\n",
    "        mu, std = norm.fit(data)\n",
    "        # Generate x values for normal CDF\n",
    "        x = np.linspace(data.min(), data.max(), 500)\n",
    "        normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "        \n",
    "        # CDF\n",
    "        ax[idx, 0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "        ax[idx, 0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "        ax[idx, 0].set_title(f\"{season}: Normal Dist. CDF\")\n",
    "        \n",
    "        # Data\n",
    "        ax[idx, 1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "        ax[idx, 1].plot(item[\"ols\"].x_range, item[\"ols\"].results.predict(sm.add_constant(item[\"ols\"].x_range)))\n",
    "        ax[idx, 1].fill_between(item[\"ols\"].x_range.flatten(), item[\"ols\"].ci_lo, item[\"ols\"].ci_hi, alpha = 0.1, color = \"black\", lw = 0)\n",
    "        ax[idx, 1].errorbar(\n",
    "            item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "            yerr = item[\"df\"][item[\"y_var_std\"]].squeeze(),\n",
    "            fmt = \"o\",\n",
    "            elinewidth = 0.5,\n",
    "            capsize = 2.5,\n",
    "            color = \"C0\",\n",
    "            alpha = 1\n",
    "        )\n",
    "        ax[idx, 1].set_title(f\"{season}: {item[\"y_var\"]} over {item[\"x_var\"]}\")\n",
    "        if idx == 3:\n",
    "            ax[idx, 1].set_xlabel(item[\"x_var\"])\n",
    "        ax[idx, 1].set_ylabel(item[\"y_var\"])\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPPe Temperature (WI) Residuals and Snowdepth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals\n",
    "Take the residuals from the gppe - temperature WI relationship (we rerun the OLS so we have the option to look at sub-seasonal relationships too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define residuals function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy, var_w_tup = None, debug = False):\n",
    "    \n",
    "    # Merge by outer index\n",
    "    df = pd.merge(\n",
    "        df_x,\n",
    "        df_y,\n",
    "        left_index = True,\n",
    "        right_index = True,\n",
    "        how = \"outer\",\n",
    "    )\n",
    "    \n",
    "    # Get vars\n",
    "    df_res = df[[var_x_tup, var_y_tup]].copy()\n",
    "    df_res.dropna(inplace = True)\n",
    "    \n",
    "    # Calc Regression\n",
    "    if logy:\n",
    "        ols_stats = ols(df_res[var_x_tup], np.log1p(df_res[var_y_tup]), n_boot_samples = global_n_boots)\n",
    "        resid = np.exp(ols_stats.results.resid) # exponentiate residuals\n",
    "        resid_lo = np.exp(ols_stats.resid_lo)\n",
    "        resid_hi = np.exp(ols_stats.resid_hi)\n",
    "    else:\n",
    "        ols_stats = ols(df_res[var_x_tup], df_res[var_y_tup], n_boot_samples = global_n_boots)\n",
    "        resid = ols_stats.results.resid\n",
    "        resid_lo = ols_stats.resid_lo\n",
    "        resid_hi = ols_stats.resid_hi\n",
    "    \n",
    "    resid.name = var_z_tup # name into multilevel header\n",
    "    resid_lo.name = (var_z_tup[0], var_z_tup[1] + \"_lo\") # name into multilevel header\n",
    "    resid_hi.name = (var_z_tup[0], var_z_tup[1] + \"_hi\") # name into multilevel header\n",
    "    resid_df = pd.concat([resid, resid_lo, resid_hi], axis = 1)\n",
    "    \n",
    "    # Merge resid back into y df\n",
    "    df_y = pd.merge(\n",
    "        df_y,\n",
    "        resid_df,\n",
    "        left_index = True,\n",
    "        right_index = True,\n",
    "        how = \"outer\",\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        # Residuals\n",
    "        plt.scatter(df_res[var_x_tup], df_res[var_y_tup])\n",
    "        pred_y = results.get_prediction(X_range) # Predict\n",
    "        if logy:\n",
    "            pred_summary = pred_y.summary_frame(alpha = 0.1)\n",
    "            pred_y = np.exp(pred_summary['mean']) # Exponentiate predictions\n",
    "            ci_lower = np.exp(pred_summary['mean_ci_lower'])\n",
    "            ci_upper = np.exp(pred_summary['mean_ci_upper'])\n",
    "        else:\n",
    "            pred_summary = pred_y.summary_frame(alpha = 0.1)\n",
    "            pred_y = pred_summary['mean']\n",
    "            ci_lower = pred_summary['mean_ci_lower']\n",
    "            ci_upper = pred_summary['mean_ci_upper']\n",
    "            \n",
    "        plt.plot(x_range, pred_y, color = \"black\", lw = 0.8, label = \"Exponential fit\")\n",
    "        plt.fill_between(x_range, ci_lower, ci_upper, color = \"black\", alpha = 0.1, lw = 0)\n",
    "        plt.show()\n",
    "        \n",
    "        # Residuals against W\n",
    "        df = pd.merge(\n",
    "            df_x,\n",
    "            df_y,\n",
    "            left_index = True,\n",
    "            right_index = True,\n",
    "            how = \"outer\",\n",
    "        )\n",
    "\n",
    "        df = df[[var_w_tup, var_z_tup]].copy()\n",
    "        df.dropna(inplace = True)\n",
    "        df = df[df.index.month.isin([3, 4, 5])]\n",
    "        \n",
    "        if logy:\n",
    "            results, ci_upper, ci_lower, predictions, x_range, X_range, *_, = ols(df[var_w_tup], np.log1p(df[var_z_tup]))\n",
    "            resid = np.exp(results.resid) # exponentiate residuals\n",
    "        else:\n",
    "            results, ci_upper, ci_lower, predictions, x_range, X_range, *_, = ols(df[var_w_tup], df[var_z_tup])\n",
    "            resid = results.resid # exponentiate residuals\n",
    "        \n",
    "        pred_y = results.get_prediction(X_range) # Predict\n",
    "        if logy:\n",
    "            pred_summary = pred_y.summary_frame(alpha = 0.1)\n",
    "            pred_y = np.exp(pred_summary['mean']) # Exponentiate predictions\n",
    "            ci_lower = np.exp(pred_summary['mean_ci_lower'])\n",
    "            ci_upper = np.exp(pred_summary['mean_ci_upper'])\n",
    "        else:\n",
    "            pred_summary = pred_y.summary_frame(alpha = 0.1)\n",
    "            pred_y = pred_summary['mean']\n",
    "            ci_lower = pred_summary['mean_ci_lower']\n",
    "            ci_upper = pred_summary['mean_ci_upper']\n",
    "\n",
    "        plt.scatter(df[var_w_tup], df[var_z_tup])\n",
    "        plt.plot(x_range, pred_y, color = \"black\", lw = 0.8, label = \"Exponential fit\")\n",
    "        plt.fill_between(x_range, ci_lower, ci_upper, color = \"black\", alpha = 0.1, lw = 0)\n",
    "        plt.show()\n",
    "        \n",
    "    return df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sites:\n",
    "    \n",
    "    ## Basic Temp\n",
    "    ## Day\n",
    "    #df_x = sites[key].cli_day\n",
    "    #df_y = sites[key].sat_day\n",
    "    #var_x_tup = (\"temperature\")\n",
    "    #var_y_tup = (\"gppe\")\n",
    "    #var_z_tup = (\"gppe_res_mean\")\n",
    "    #sites[key].sat_day = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = True, debug = False)\n",
    "    ## Week\n",
    "    #df_x = sites[key].cli_week\n",
    "    #df_y = sites[key].sat_week\n",
    "    #var_x_tup = (\"temperature\", \"mean\")\n",
    "    #var_y_tup = (\"gppe\", \"auc\")\n",
    "    #var_z_tup = (\"gppe\", \"auc_res_mean\")\n",
    "    #sites[key].sat_week = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = True, debug = False)\n",
    "    ## Month\n",
    "    #df_x = sites[key].cli_month\n",
    "    #df_y = sites[key].sat_month\n",
    "    #var_x_tup = (\"temperature\", \"mean\")\n",
    "    #var_y_tup = (\"gppe\", \"auc\")\n",
    "    #var_z_tup = (\"gppe\", \"auc_res_mean\")\n",
    "    #sites[key].sat_month = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = True, debug = False)\n",
    "    ## Season\n",
    "    #df_x = sites[key].cli_season\n",
    "    #df_y = sites[key].sat_season\n",
    "    #var_x_tup = (\"temperature\", \"mean\")\n",
    "    #var_y_tup = (\"gppe\", \"auc\")\n",
    "    #var_z_tup = (\"gppe\", \"auc_res_mean\")\n",
    "    #sites[key].sat_season = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = False, debug = False)\n",
    "    ## Annual\n",
    "    #df_x = sites[key].cli_annual\n",
    "    #df_y = sites[key].sat_annual\n",
    "    #var_x_tup = (\"temperature\", \"mean\")\n",
    "    #var_y_tup = (\"gppe\", \"auc\")\n",
    "    #var_z_tup = (\"gppe\", \"auc_res_mean\")\n",
    "    #sites[key].sat_annual = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = False, debug = False)\n",
    "    \n",
    "    # Warmth Index\n",
    "    # Season\n",
    "    df_x = sites[key].cli_season\n",
    "    df_y = sites[key].sat_season\n",
    "    var_x_tup = (\"temperature\", \"wi\")\n",
    "    var_y_tup = (\"gppe\", \"auc\")\n",
    "    var_z_tup = (\"gppe\", \"auc_res_wi\")\n",
    "    sites[key].sat_season = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = False, debug = False)\n",
    "    # Annual\n",
    "    df_x = sites[key].cli_annual\n",
    "    df_y = sites[key].sat_annual\n",
    "    var_x_tup = (\"temperature\", \"wi\")\n",
    "    var_y_tup = (\"gppe\", \"auc\")\n",
    "    var_z_tup = (\"gppe\", \"auc_res_wi\")\n",
    "    sites[key].sat_annual = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = False, debug = False)\n",
    "    \n",
    "    ## GDD\n",
    "    ## Day\n",
    "    #df_x = sites[key].cli_day\n",
    "    #df_y = sites[key].sat_day\n",
    "    #var_x_tup = (\"temperature\")\n",
    "    #var_y_tup = (\"gppe\")\n",
    "    #var_z_tup = (\"gppe_res_gdd\")\n",
    "    #sites[key].sat_day = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = True, debug = False)\n",
    "    ## Week\n",
    "    #df_x = sites[key].cli_week\n",
    "    #df_y = sites[key].sat_week\n",
    "    #var_x_tup = (\"temperature\", \"gdd\")\n",
    "    #var_y_tup = (\"gppe\", \"auc\")\n",
    "    #var_z_tup = (\"gppe\", \"auc_res_gdd\")\n",
    "    #sites[key].sat_week = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = True, debug = False)\n",
    "    ## Month\n",
    "    #df_x = sites[key].cli_month\n",
    "    #df_y = sites[key].sat_month\n",
    "    #var_x_tup = (\"temperature\", \"gdd\")\n",
    "    #var_y_tup = (\"gppe\", \"auc\")\n",
    "    #var_z_tup = (\"gppe\", \"auc_res_gdd\")\n",
    "    #sites[key].sat_month = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = True, debug = False)\n",
    "    ## Season\n",
    "    #df_x = sites[key].cli_season\n",
    "    #df_y = sites[key].sat_season\n",
    "    #var_x_tup = (\"temperature\", \"gdd\")\n",
    "    #var_y_tup = (\"gppe\", \"auc\")\n",
    "    #var_z_tup = (\"gppe\", \"auc_res_gdd\")\n",
    "    #sites[key].sat_season = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = False, debug = False)\n",
    "    ## Annual\n",
    "    #df_x = sites[key].cli_annual\n",
    "    #df_y = sites[key].sat_annual\n",
    "    #var_x_tup = (\"temperature\", \"gdd\")\n",
    "    #var_y_tup = (\"gppe\", \"auc\")\n",
    "    #var_z_tup = (\"gppe\", \"auc_res_gdd\")\n",
    "    #sites[key].sat_annual = residuals(df_x, df_y, var_x_tup, var_y_tup, var_z_tup, logy = False, debug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"snowdepth\", \"mean\")\n",
    "y_col = (\"gppe\", \"auc_res_wi\")\n",
    "y_col_lo = (\"gppe\", \"auc_res_wi_lo\")\n",
    "y_col_hi = (\"gppe\", \"auc_res_wi_hi\")\n",
    "var_names = [\"snowdepth\", \"gppe\", \"lo\", \"hi\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = pd.merge(left = item.sat_annual.copy(), right = item.cli_annual.copy(), left_index = True, right_index = True, how = \"inner\")\n",
    "    #df.index = datetime_to_decimal_years(df.index)\n",
    "    #df = df.reset_index()\n",
    "    df = df[[x_col, y_col, y_col_lo, y_col_hi]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "\n",
    "    # Stats\n",
    "    sample_std = np.std(df[y_var], axis = 0)\n",
    "    ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    #kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "    #ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "    \n",
    "    # Add entry to stats dict\n",
    "    stats_dict[key][\"gppe-snow_annual\"] = {\n",
    "        \"df\": df,\n",
    "        \"x_var\": var_names[0],\n",
    "        \"y_var\": var_names[1],\n",
    "        \"y_var_lo\": var_names[2],\n",
    "        \"y_var_hi\": var_names[3],\n",
    "        \"sample_std\": sample_std,\n",
    "        \"ols\": ols_stats,\n",
    "        #\"kt\": kt_stats,\n",
    "        #\"ts\": ts_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (key, item) in enumerate(stats_dict.items()):\n",
    "    \n",
    "    item = item[\"gppe-snow_annual\"]\n",
    "    ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "    ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "    #ts_slope = item[\"ts\"].slope\n",
    "    #kt_tau = item[\"kt\"].tau\n",
    "    #kt_pval = item[\"kt\"].pval\n",
    "    \n",
    "    print(f\"At {key}, {item[\"y_var\"]} over {item[\"x_var\"]} reported (adj.r2:{ols_r2:.2f}, p:{ols_pval:.2f})\")\n",
    "    \n",
    "    print(item[\"ols\"].results.summary())\n",
    "    print(\"\")\n",
    "     \n",
    "    \n",
    "    # Check Data Distribution\n",
    "    data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "    # Fit a normal distribution\n",
    "    mu, std = norm.fit(data)\n",
    "    # Generate x values for normal CDF\n",
    "    x = np.linspace(data.min(), data.max(), 500)\n",
    "    normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (12, 4), layout = \"constrained\")\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    # CDF\n",
    "    ax[0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "    ax[0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "    ax[0].set_title(\"Normal Dist. CDF\")\n",
    "    \n",
    "    # Data\n",
    "    ax[1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "    ax[1].plot(item[\"ols\"].x_range, item[\"ols\"].results.predict(sm.add_constant(item[\"ols\"].x_range)))\n",
    "    ax[1].fill_between(item[\"ols\"].x_range.flatten(), item[\"ols\"].ci_lo, item[\"ols\"].ci_hi, alpha = 0.1, color = \"black\", lw = 0)\n",
    "    ax[1].errorbar(\n",
    "        item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "        yerr = np.array([(item[\"df\"][item[\"y_var\"]].values - item[\"df\"][item[\"y_var_lo\"]].values).flatten(), (item[\"df\"][item[\"y_var_hi\"]].values - item[\"df\"][item[\"y_var\"]].values).flatten()]),#np.array([pd.Series((item[\"df\"][item[\"y_var\"]].values - item[\"df\"][item[\"y_var_lo\"]].values).flatten(), index = item[\"df\"].index), pd.Series((item[\"df\"][item[\"y_var_hi\"]].values - item[\"df\"][item[\"y_var\"]].values).flatten(), index = item[\"df\"].index)]), # pandas cols are different names so cannot subract one anotger. so take values and then make back into pd.series\n",
    "        fmt = \"o\",\n",
    "        elinewidth = 0.5,\n",
    "        capsize = 2.5,\n",
    "        color = \"C0\",\n",
    "        alpha = 1\n",
    "    )\n",
    "    ax[1].set_title(key)\n",
    "    ax[1].set_xlabel(item[\"x_var\"])\n",
    "    ax[1].set_ylabel(item[\"y_var\"])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag(df, lag, month_filter):\n",
    "    df[x_var] = df[x_var].shift(lag)\n",
    "    df.dropna(inplace = True)\n",
    "    df = df[df.index.month.isin(month_filter)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col = (\"snowdepth\", \"mean\")\n",
    "y_col = (\"gppe\", \"auc_res_wi\")\n",
    "y_col_lo = (\"gppe\", \"auc_res_wi_lo\")\n",
    "y_col_hi = (\"gppe\", \"auc_res_wi_hi\")\n",
    "var_names = [\"snowdepth\", \"gppe\", \"lo\", \"hi\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "    \n",
    "    # Get Data\n",
    "    df = pd.merge(left = item.sat_season.copy(), right = item.cli_season.copy(), left_index = True, right_index = True, how = \"inner\")\n",
    "    #df.index = datetime_to_decimal_years(df.index)\n",
    "    #df = df.reset_index()\n",
    "    df = df[[x_col, y_col, y_col_lo, y_col_hi]]\n",
    "    df = df.droplevel(level = 0, axis = 1)\n",
    "    df.columns = [var_names]\n",
    "    df.dropna(inplace = True)\n",
    "    x_var = var_names[0] # drop 2nd item in var tuples\n",
    "    y_var = var_names[1]\n",
    "    y_var_std = var_names[2]\n",
    "    \n",
    "    seasons = {\n",
    "        #\"summer\": df[df.index.month.isin([6, 7, 8])], # no snow in summer\n",
    "        #\"winter\": df[df.index.month.isin([12, 1, 2])], # no gppe in winter\n",
    "        \"spring\": df[df.index.month.isin([3, 4, 5])],\n",
    "        \"spring lag\": lag(df.copy(), 1, [3, 4, 5]), # must be copy as shift is an in place function\n",
    "        \"autumn\": df[df.index.month.isin([9, 10, 11])]\n",
    "    }\n",
    "    \n",
    "    stats_dict[key][\"gppe-snow_season\"] = {}\n",
    "    for season, df in seasons.items():\n",
    "\n",
    "        # Stats\n",
    "        sample_std = np.std(df[y_var], axis = 0)\n",
    "        ols_stats = ols(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "        #kt_stats = kendall_trend(df[x_var], df[y_var])\n",
    "        #ts_stats = theilsen(df[x_var], df[y_var], n_boot_samples = global_n_boots)\n",
    "        \n",
    "        # Add entry to stats dict\n",
    "        stats_dict[key][\"gppe-snow_season\"][season] = {\n",
    "            \"df\": df,\n",
    "            \"x_var\": var_names[0],\n",
    "            \"y_var\": var_names[1],\n",
    "            \"y_var_lo\": var_names[2],\n",
    "            \"y_var_hi\": var_names[3],\n",
    "            \"sample_std\": sample_std,\n",
    "            \"ols\": ols_stats,\n",
    "            #\"kt\": kt_stats,\n",
    "            #\"ts\": ts_stats\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site, entry in stats_dict.items():\n",
    "\n",
    "    # Test Plots\n",
    "    fig, ax = plt.subplots(3, 2, figsize = (12, 4*3), layout = \"constrained\")\n",
    "    \n",
    "    for idx, (season, item) in enumerate(entry[\"gppe-snow_season\"].items()):\n",
    "        \n",
    "        ols_r2 = item[\"ols\"].results.rsquared_adj.item()\n",
    "        ols_pval = item[\"ols\"].results.pvalues[(item[\"x_var\"]),].item()\n",
    "        #ts_slope = item[\"ts\"].slope\n",
    "        #n = item[\"y_std\"].count\n",
    "        #kt_tau = item[\"kt\"].tau\n",
    "        #kt_pval = item[\"kt\"].pval\n",
    "            \n",
    "        print(f\"At {site} in {season}, {item[\"y_var\"]} over {item[\"x_var\"]} reported (adj.r2:{ols_r2:.2f}, p:{ols_pval:.2f})\")\n",
    "        \n",
    "        print(item[\"ols\"].results.summary())\n",
    "        print(\"\")\n",
    "        \n",
    "        # Check Data Distribution\n",
    "        data = item[\"df\"][item[\"y_var\"]].squeeze()\n",
    "        # Fit a normal distribution\n",
    "        mu, std = norm.fit(data)\n",
    "        # Generate x values for normal CDF\n",
    "        x = np.linspace(data.min(), data.max(), 500)\n",
    "        normal_cdf = norm.cdf(x, loc = mu, scale = std)\n",
    "\n",
    "        \n",
    "        # CDF\n",
    "        ax[idx, 0].ecdf(data, linestyle = \"\", marker = \".\", color = \"C0\") # Empirical CDF\n",
    "        ax[idx, 0].plot(x, normal_cdf, color = \"C3\", linewidth = 1.5) # Normal CDF line\n",
    "        ax[idx, 0].set_title(f\"{season}: Normal Dist. CDF\")\n",
    "        \n",
    "        # Data\n",
    "        ax[idx, 1].scatter(item[\"df\"][item[\"x_var\"]], item[\"df\"][item[\"y_var\"]])\n",
    "        ax[idx, 1].plot(item[\"ols\"].x_range, item[\"ols\"].results.predict(sm.add_constant(item[\"ols\"].x_range)))\n",
    "        ax[idx, 1].fill_between(item[\"ols\"].x_range.flatten(), item[\"ols\"].ci_lo, item[\"ols\"].ci_hi, alpha = 0.1, color = \"black\", lw = 0)\n",
    "        ax[idx, 1].errorbar(\n",
    "            item[\"df\"][item[\"x_var\"]].squeeze(), item[\"df\"][item[\"y_var\"]].squeeze(),\n",
    "            yerr = np.array([pd.Series((item[\"df\"][item[\"y_var\"]].values - item[\"df\"][item[\"y_var_lo\"]].values).flatten(), index = item[\"df\"].index), pd.Series((item[\"df\"][item[\"y_var_hi\"]].values - item[\"df\"][item[\"y_var\"]].values).flatten(), index = item[\"df\"].index)]), # pandas cols are different names so cannot subract one anotger. so take values and then make back into pd.series\n",
    "            fmt = \"o\",\n",
    "            elinewidth = 0.5,\n",
    "            capsize = 2.5,\n",
    "            color = \"C0\",\n",
    "            alpha = 1\n",
    "        )\n",
    "        ax[idx, 1].set_title(f\"{season}: {item[\"y_var\"]} over {item[\"x_var\"]}\")\n",
    "        if idx == 3:\n",
    "            ax[idx, 1].set_xlabel(item[\"x_var\"])\n",
    "        ax[idx, 1].set_ylabel(item[\"y_var\"])\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPPe Phenology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vars = [\"gpp\", \"gppe\", \"ndvi\"]\n",
    "\n",
    "# Get Data\n",
    "df_day = pd.concat([sites[site].sat_day.copy() for site in sites], axis = 0)\n",
    "df_day.index = df_day.index.day_of_year # Index to day of year\n",
    "df_day[\"week\"] = pd.cut(df_day.index, bins = np.arange(0, 366, 7), right = False, labels = False) # Add Week No. Column\n",
    "\n",
    "df_week = []\n",
    "for var in y_vars:\n",
    "    \n",
    "    # Z-score\n",
    "    df_day[var] = zscore(df_day[var], nan_policy = \"omit\")\n",
    "    \n",
    "    # Group by Week\n",
    "    df_week_loop = df_day[[var, \"week\"]].groupby(\"week\")[var].agg([\"mean\", lambda x: np.nanpercentile(x, 25), lambda x: np.nanpercentile(x, 75)])\n",
    "    df_week_loop.columns = pd.MultiIndex.from_tuples([(var, \"mean\"), (var, \"q25\"), (var, \"q75\")]) # Make multi-index col names\n",
    "   \n",
    "    df_week.append(df_week_loop)\n",
    "\n",
    "df_week = pd.concat(df_week, axis = 1)\n",
    "\n",
    "# X positions\n",
    "df_week.index = df_week.index.to_numpy() * 7 + 3  # Mid-point of week\n",
    "\n",
    "# Stats\n",
    "#mean_columns = df_week.loc[:, (slice(None), 'mean')] # just the means\n",
    "#\n",
    "#levene_res = levene(*[mean_columns[col] for col in mean_columns])\n",
    "#print(\"Levene:\", levene_res.statistic, levene_res.pvalue)\n",
    "#\n",
    "#anova_res = f_oneway(*[mean_columns[col] for col in mean_columns]) # unpack from list comprehension to pass as individual arguments\n",
    "#print(\"ANOVA F-statistic:\", anova_res.statistic, anova_res.pvalue)\n",
    "#\n",
    "## Store\n",
    "stats_dict_all = {\n",
    "    \"phenology\": {\n",
    "        \"df_day\": df_day,\n",
    "        \"df_week\": df_week,\n",
    "        \"y_vars\": y_vars,\n",
    "#        \"levene\": levene_res,\n",
    "#        \"anova\": anova_res\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vars = [\"gpp\", \"gppe\", \"ndvi\"]\n",
    "\n",
    "for idx, (key, item) in enumerate(sites.items()):\n",
    "\n",
    "    # Get Data\n",
    "    df_day = item.sat_day.copy()\n",
    "\n",
    "    df_day.index = df_day.index.day_of_year # Index to day of year\n",
    "    df_day[\"week\"] = pd.cut(df_day.index, bins = np.arange(0, 366, 7), right = False, labels = False) # Add Week No. Column\n",
    "\n",
    "    df_week = []\n",
    "    for var in y_vars:\n",
    "        \n",
    "        # Z-score\n",
    "        df_day[var] = zscore(df_day[var], nan_policy = \"omit\")\n",
    "        \n",
    "        # Group by Week\n",
    "        df_week_loop = df_day[[var, \"week\"]].groupby(\"week\")[var].agg([\"mean\", lambda x: np.nanpercentile(x, 25), lambda x: np.nanpercentile(x, 75)])\n",
    "        df_week_loop.columns = pd.MultiIndex.from_tuples([(var, \"mean\"), (var, \"q25\"), (var, \"q75\")]) # Make multi-index col names\n",
    "    \n",
    "        df_week.append(df_week_loop)\n",
    "\n",
    "    df_week = pd.concat(df_week, axis = 1)\n",
    "\n",
    "    # X positions\n",
    "    df_week.index = df_week.index.to_numpy() * 7 + 3  # Mid-point of week\n",
    "    #df_week.dropna(inplace = True, axis = 1)\n",
    "    \n",
    "    # Stats\n",
    "    #mean_columns = df_week.loc[:, (slice(None), 'mean')] # just the means\n",
    "    #levene_res = levene(*[mean_columns[col] for col in mean_columns])\n",
    "    #anova_res = f_oneway(*[mean_columns[col] for col in mean_columns]) # unpack from list comprehension to pass as individual arguments\n",
    "    #\n",
    "    #print(f\"At {key}\\nLevene: {levene_res.statistic:.2f}, p={levene_res.pvalue:.2f} (non-sig. pval = equal variances)\\nANOVA F: {anova_res.statistic:.2f}, p={anova_res.pvalue:.2f}\\n\")\n",
    "\n",
    "    # Store\n",
    "    stats_dict[key][\"phenology\"] = {\n",
    "        \"df_day\": df_day,\n",
    "        \"df_week\": df_week,\n",
    "        \"y_vars\": y_vars,\n",
    "    #    \"levene\": levene_res,\n",
    "    #    \"anova\": anova_res\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Data\n",
    "#df = sites[\"se_st1\"].sat_day.copy()\n",
    "#\n",
    "# # Z-score\n",
    "#for var in [\"gpp\", \"gppe\", \"ndvi\"]:\n",
    "#    df[var] = zscore(df[var], nan_policy = \"omit\")\n",
    "#\n",
    "## Move index to col\n",
    "#df = df.reset_index().rename(columns={'index': 'date'})\n",
    "#\n",
    "## Melt\n",
    "#df_long = df.melt(\n",
    "#    id_vars = [\"date\"], # columns to keep\n",
    "#    value_vars = [\"gppe\", \"ndvi\", \"gpp\"], # columns to unpivot\n",
    "#    var_name = \"method\",\n",
    "#    value_name = \"z_score\"  # your standardized phenology value\n",
    "#)\n",
    "#\n",
    "## Unpack Date\n",
    "#df_long[\"date\"] = pd.to_datetime(df_long[\"date\"])\n",
    "#df_long[\"year\"] = df_long[\"date\"].dt.year\n",
    "#df_long[\"week\"] = df_long[\"date\"].dt.isocalendar().week\n",
    "#\n",
    "#df_long = df_long.dropna(subset = [\"z_score\"])\n",
    "#\n",
    "#df = df_long\n",
    "#\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.formula.api import mixedlm\n",
    "#\n",
    "## Convert method to categorical (if not already)\n",
    "#df['method'] = df['method'].astype('category')\n",
    "#\n",
    "## Specify the formula: z-score ~ method, with random intercepts for year and week\n",
    "## Random intercept for year and week nested within year\n",
    "#\n",
    "## Create a combined grouping for week nested in year\n",
    "#df['year_week'] = df['year'].astype(str) + '_' + df['week'].astype(str)\n",
    "#\n",
    "## Fit the linear mixed effect model\n",
    "#model = mixedlm(\"z_score ~ method\", df, groups=df[\"year\"], \n",
    "#                re_formula=\"1\")\n",
    "#result = model.fit()\n",
    "#print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats.pkl\", \"wb\") as f:\n",
    "    dill.dump(stats_dict, f)\n",
    "    \n",
    "with open(\"stats_combined.pkl\", \"wb\") as f:\n",
    "    dill.dump(stats_dict_all, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
