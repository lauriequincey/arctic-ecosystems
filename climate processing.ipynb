{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Management\n",
    "from dataclasses import dataclass, field\n",
    "from collections import namedtuple\n",
    "from typing import List, Any\n",
    "import pickle\n",
    "\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_smhi(path, skiprows, usecols, rename):\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        skiprows = skiprows, sep = \";\", parse_dates = True,\n",
    "        usecols = usecols\n",
    "        )\n",
    "    df.columns = rename\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]) # Convert to datetimes\n",
    "    return df\n",
    "\n",
    "def import_fmi(path, skiprows, usecols, rename):\n",
    "    #converters = {col: convert_dashes for col in usecols}  # Apply convert_dashes to each column in usecols\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        skiprows = skiprows, sep = \",\", parse_dates = True,\n",
    "        usecols = usecols,\n",
    "    #    converters = converters\n",
    "    na_values = \"-\"\n",
    "        )\n",
    "    df.columns = rename\n",
    "    #df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]]) # Convert to datetimes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arctic Atmospheric Indices (NAO and AO)\n",
    "nao = pd.read_csv(\"../data/atmosphere/nao/norm.daily.nao.cdas.z500.19500101_current.csv\").rename(columns = {\"nao_index_cdas\": \"nao\"})\n",
    "ao = pd.read_csv(\"../data/atmosphere/ao/norm.daily.ao.cdas.z1000.19500101_current.csv\").rename(columns = {\"ao_index_cdas\": \"ao\"})\n",
    "\n",
    "# SMHI - Swedish Meteorological Institute Data for Abisko-Stordalen Palsa Bog\n",
    "# We're importing both the manual and automatic station data. Why? The auto station data has lots of data gaps but runs to present day. The manual has less data gaps but stops in about 2022. We'll merge them in a bit.\n",
    "# Quality Flag:\n",
    "# Green (G) = Checked and approved values.\n",
    "# Yellow (Y) = Suspicious or aggregated values. Roughly checked archival data and unchecked real-time data (last 2 hours).\n",
    "\n",
    "se_sto_precip_manual = import_smhi(\n",
    "    path = \"../data/climate/abisko-stordalen-palsa-bog/precipitation/smhi-opendata_5_188800_20250418_163156.csv\",\n",
    "    skiprows = 9,\n",
    "    usecols = [\"Representativt dygn\", \"Nederbördsmängd\", \"Kvalitet\"],\n",
    "    rename = [\"date\", \"precipitation\", \"quality_flag\"]\n",
    ")\n",
    "se_sto_precip_auto = import_smhi(\n",
    "    path = \"../data/climate/abisko-stordalen-palsa-bog/precipitation/smhi-opendata_5_188790_20250418_163211.csv\",\n",
    "    skiprows = 9,\n",
    "    usecols = [\"Representativt dygn\", \"Nederbördsmängd\", \"Kvalitet\"],\n",
    "    rename = [\"date\", \"precipitation\", \"quality_flag\"]\n",
    ")\n",
    "se_sto_temp_manual = import_smhi(\n",
    "    path = \"../data/climate/abisko-stordalen-palsa-bog/temperature/smhi-opendata_2_188800_20250418_130808.csv\",\n",
    "    skiprows = 9,\n",
    "    usecols = [\"Representativt dygn\", \"Lufttemperatur\", \"Kvalitet\"],\n",
    "    rename = [\"date\", \"temperature\", \"quality_flag\"]\n",
    ")\n",
    "se_sto_temp_auto = import_smhi(\n",
    "    path = \"../data/climate/abisko-stordalen-palsa-bog/temperature/smhi-opendata_2_188790_20250418_130625.csv\",\n",
    "    skiprows = 9,\n",
    "    usecols = [\"Representativt dygn\", \"Lufttemperatur\", \"Kvalitet\"],\n",
    "    rename = [\"date\", \"temperature\", \"quality_flag\"]\n",
    ")\n",
    "se_sto_snowdepth_manual = import_smhi(\n",
    "    path = \"../data/climate/abisko-stordalen-palsa-bog/snowdepth/smhi-opendata_8_40_188800_20250418_164039.csv\",\n",
    "    skiprows = 9,\n",
    "    usecols = [\"Datum\", \"Snödjup\", \"Kvalitet\"],\n",
    "    rename = [\"date\", \"snowdepth\", \"quality_flag\"]\n",
    ")\n",
    "se_sto_snowdepth_auto = import_smhi(\n",
    "    path = \"../data/climate/abisko-stordalen-palsa-bog/snowdepth/smhi-opendata_8_40_188790_20250418_164059.csv\",\n",
    "    skiprows = 11,\n",
    "    usecols = [\"Datum\", \"Snödjup\", \"Kvalitet\"],\n",
    "    rename = [\"date\", \"snowdepth\", \"quality_flag\"]\n",
    ")\n",
    "\n",
    "# FMI - Finnish Meteorological Institute Data for Kenttarova, Sodankyla, and Varrio\n",
    "fi_ken_clim = import_fmi(\n",
    "    path = \"../data/climate/kenttarova/Kittilä Kenttärova_ 1.1.1970 - 14.5.2025.csv\",\n",
    "    skiprows = 0,\n",
    "    usecols = [\"Year\", \"Month\", \"Day\", \"Precipitation amount [mm]\", \"Snow depth [cm]\", \"Average temperature [°C]\"],\n",
    "    rename = [\"year\", \"month\", \"day\", \"precipitation\", \"snowdepth\", \"temperature\"]\n",
    "    )\n",
    "fi_sod_clim = import_fmi(\n",
    "    path = \"../data/climate/sodankyla/Sodankylä Tähtelä_ 1.1.1891 - 14.5.2025.csv\",\n",
    "    skiprows = 0,\n",
    "    usecols = [\"Year\", \"Month\", \"Day\", \"Precipitation amount [mm]\", \"Snow depth [cm]\", \"Average temperature [°C]\"],\n",
    "    rename = [\"year\", \"month\", \"day\", \"precipitation\", \"snowdepth\", \"temperature\"]\n",
    "    )\n",
    "fi_var_clim = import_fmi(\n",
    "    path = \"../data/climate/varrio/Salla Värriötunturi_ 1.1.1971 - 14.5.2025.csv\",\n",
    "    skiprows = 0,\n",
    "    usecols = [\"Year\", \"Month\", \"Day\", \"Precipitation amount [mm]\", \"Snow depth [cm]\", \"Average temperature [°C]\"],\n",
    "    rename = [\"year\", \"month\", \"day\", \"precipitation\", \"snowdepth\", \"temperature\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "def cleaner(df, variable_name):\n",
    "    df[variable_name] = df[variable_name].replace([-9999.0, \"Y\"], np.nan) # Replace visible/known missing values\n",
    "    df = df.reindex(pd.date_range(start = df.index[0], end = df.index[-1], freq = \"D\")) # Reindex to deal with hidden missing values\n",
    "    #df.index.name = \"date\" # put the name of the index column back!\n",
    "    df = df[[variable_name]].copy() # Select Variables\n",
    "    return df\n",
    "\n",
    "def prep_smhi(df_manual, df_auto):\n",
    "    df_manual = df_manual.set_index(\"date\", drop = False) # Make index\n",
    "    df_manual.rename_axis(None, inplace = True) # Remove the index column header (avoids column header name confusion)\n",
    "    df_manual = pd.concat([cleaner(df_manual[[column]].copy(), column) for column in df_manual.columns], axis = 1) # Clean and reassign dataframe\n",
    "\n",
    "    df_auto = df_auto.set_index(\"date\", drop = False)\n",
    "    df_auto.rename_axis(None, inplace = True) # Remove the index column header (avoids column header name confusion)\n",
    "    df_auto = pd.concat([cleaner(df_auto[[column]].copy(), column) for column in df_auto.columns], axis = 1) # Clean and reassign dataframe\n",
    "\n",
    "    return df_manual.combine_first(df_auto)\n",
    "\n",
    "def prep_fmi(df):\n",
    "    # Combine into datetime\n",
    "    df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "    df = df[[\"date\", \"precipitation\", \"temperature\", \"snowdepth\"]]\n",
    "    \n",
    "    # Different variables are taken at different times on the same day. This means our dataset has multiple rows for each day so we merge each row based on date, overwriting any NaNs. \n",
    "    df = df.groupby(\"date\", as_index = False).agg(lambda x: x.dropna().iloc[0] if not x.dropna().empty else np.nan)\n",
    "    \n",
    "    # Make date the index\n",
    "    df = df.set_index(\"date\", drop = False) \n",
    "    df.rename_axis(None, inplace = True) # Remove the index column header (avoids column header name confusion)\n",
    "\n",
    "    # Clean\n",
    "    df = pd.concat([cleaner(df[[column]].copy(), column) for column in df.columns], axis = 1) # Clean and reassign dataframe\n",
    "\n",
    "    # Clip to FluxSat date range\n",
    "    #df = df.loc[(df[\"date\"] >= fluxsat_start) & (df[\"date\"] < fluxsat_end)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and Merge NAO and AO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>nao</th>\n",
       "      <th>ao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1950-01-01</th>\n",
       "      <td>1950-01-01</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>-2.511443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950-01-02</th>\n",
       "      <td>1950-01-02</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>-1.505500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950-01-03</th>\n",
       "      <td>1950-01-03</td>\n",
       "      <td>-0.416000</td>\n",
       "      <td>-1.173238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950-01-04</th>\n",
       "      <td>1950-01-04</td>\n",
       "      <td>-0.616000</td>\n",
       "      <td>-1.250540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950-01-05</th>\n",
       "      <td>1950-01-05</td>\n",
       "      <td>-0.261000</td>\n",
       "      <td>-0.125743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-03</th>\n",
       "      <td>2025-04-03</td>\n",
       "      <td>-0.456126</td>\n",
       "      <td>0.338938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-04</th>\n",
       "      <td>2025-04-04</td>\n",
       "      <td>-0.527314</td>\n",
       "      <td>-0.273034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-05</th>\n",
       "      <td>2025-04-05</td>\n",
       "      <td>-0.634172</td>\n",
       "      <td>-0.722893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-06</th>\n",
       "      <td>2025-04-06</td>\n",
       "      <td>-0.598284</td>\n",
       "      <td>-0.835127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-07</th>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>-0.413966</td>\n",
       "      <td>-0.852780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27491 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date       nao        ao\n",
       "1950-01-01 1950-01-01  0.365000 -2.511443\n",
       "1950-01-02 1950-01-02  0.096000 -1.505500\n",
       "1950-01-03 1950-01-03 -0.416000 -1.173238\n",
       "1950-01-04 1950-01-04 -0.616000 -1.250540\n",
       "1950-01-05 1950-01-05 -0.261000 -0.125743\n",
       "...               ...       ...       ...\n",
       "2025-04-03 2025-04-03 -0.456126  0.338938\n",
       "2025-04-04 2025-04-04 -0.527314 -0.273034\n",
       "2025-04-05 2025-04-05 -0.634172 -0.722893\n",
       "2025-04-06 2025-04-06 -0.598284 -0.835127\n",
       "2025-04-07 2025-04-07 -0.413966 -0.852780\n",
       "\n",
       "[27491 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make date column (as a datetime object)\n",
    "nao['date'] = pd.to_datetime(nao[['year', 'month', 'day']])\n",
    "ao['date'] = pd.to_datetime(ao[['year', 'month', 'day']])\n",
    "\n",
    "# Merge on date column and select only the date and index columns to avoid duplicates\n",
    "atmos = nao[[\"date\", \"nao\"]].merge(right = ao[[\"date\", \"ao\"]], left_on = \"date\", right_on = \"date\")\n",
    "\n",
    "# Make date the index\n",
    "atmos = atmos.set_index(\"date\", drop = False) \n",
    "atmos.rename_axis(None, inplace = True) # Remove the index column header (avoids column header name confusion)\n",
    "\n",
    "# This data is snazzy and nicely prepared so we don't need to clean it (huzzah!)\n",
    "\n",
    "atmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and Merge SMHI data for Abisko-Stordalen Palsa Bog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurie\\AppData\\Local\\Temp\\ipykernel_2032\\3317781567.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[variable_name] = df[variable_name].replace([-9999.0, \"Y\"], np.nan) # Replace visible/known missing values\n",
      "C:\\Users\\Laurie\\AppData\\Local\\Temp\\ipykernel_2032\\3317781567.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[variable_name] = df[variable_name].replace([-9999.0, \"Y\"], np.nan) # Replace visible/known missing values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>temperature</th>\n",
       "      <th>snowdepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1913-01-13</th>\n",
       "      <td>1913-01-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913-01-14</th>\n",
       "      <td>1913-01-14</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-19.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913-01-15</th>\n",
       "      <td>1913-01-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913-01-16</th>\n",
       "      <td>1913-01-16</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-19.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913-01-17</th>\n",
       "      <td>1913-01-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-27</th>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-28</th>\n",
       "      <td>2024-12-28</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-29</th>\n",
       "      <td>2024-12-29</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.7</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40896 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  precipitation  temperature  snowdepth\n",
       "1913-01-13 1913-01-13            0.0        -14.0        NaN\n",
       "1913-01-14 1913-01-14            0.1        -19.2        NaN\n",
       "1913-01-15 1913-01-15            0.0        -20.3        NaN\n",
       "1913-01-16 1913-01-16            0.1        -19.9        NaN\n",
       "1913-01-17 1913-01-17            0.0        -10.3        NaN\n",
       "...               ...            ...          ...        ...\n",
       "2024-12-27 2024-12-27            5.0          1.5       0.58\n",
       "2024-12-28 2024-12-28            0.4          2.1       0.56\n",
       "2024-12-29 2024-12-29            0.5         -1.5       0.55\n",
       "2024-12-30 2024-12-30            0.0         -2.3       0.57\n",
       "2024-12-31 2024-12-31            0.0         -4.7       0.58\n",
       "\n",
       "[40896 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add index, clean data (by \"Y\" quality flag), and union the cleaned manual and auto dataframes\n",
    "se_sto_precip = prep_smhi(se_sto_precip_manual, se_sto_precip_auto)\n",
    "se_sto_temp = prep_smhi(se_sto_temp_manual, se_sto_temp_auto)\n",
    "se_sto_snowdepth = prep_smhi(se_sto_snowdepth_manual, se_sto_snowdepth_auto)\n",
    "\n",
    "# Merge SMHI data on date and select only the climate variables\n",
    "se_sto_clim = se_sto_precip[[\"date\", \"precipitation\"]].merge(right = se_sto_temp[[\"date\", \"temperature\"]], left_on = \"date\", right_on = \"date\").merge(right = se_sto_snowdepth[[\"date\", \"snowdepth\"]], left_on = \"date\", right_on = \"date\")\n",
    "se_sto_clim = se_sto_clim.set_index(\"date\", drop = False) # Make index\n",
    "\n",
    "# Reclean just in case we have a few missing dates (mainly for snowdepth)\n",
    "se_sto_clim = pd.concat([cleaner(se_sto_clim[[column]].copy(), column) for column in se_sto_clim.columns], axis = 1) # Clean and reassign dataframe\n",
    "\n",
    "se_sto_clim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean FMI Data for Kenttarova, Sodankyla, and Varrio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_ken_clim = prep_fmi(fi_ken_clim)\n",
    "fi_sod_clim = prep_fmi(fi_sod_clim)\n",
    "fi_var_clim = prep_fmi(fi_var_clim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(se_sto_clim[\"precipitation\"].resample(\"MS\").mean().rolling(12, center = True, min_periods = 8).mean()) # se_sto_clim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmos.to_csv(f\"../data/processed/atmos.csv\")\n",
    "se_sto_clim.to_csv(f\"../data/processed/se_sto_climate.csv\")\n",
    "fi_ken_clim.to_csv(f\"../data/processed/fi_ken_climate.csv\")\n",
    "fi_sod_clim.to_csv(f\"../data/processed/fi_sod_climate.csv\")\n",
    "fi_var_clim.to_csv(f\"../data/processed/fi_var_climate.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
